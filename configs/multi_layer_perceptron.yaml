algorithm: mlp
batch_size: 128
dropout_rate: 0.5
epochs: 1
hidden_size: 128
learning_rate: 0.001
normalize: True
save_model: saved_models/multi_layer_perceptron.pt
weight_decay: 0
